"""
Module contenant les classes de mod√®les pour la classification multilabel.
Impl√©mente diff√©rentes architectures : TF-IDF + LogReg, TF-IDF + MiniLM + LogReg
"""

import numpy as np
import pickle
from typing import Dict, List, Tuple, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sentence_transformers import SentenceTransformer
from scipy.sparse import hstack, csr_matrix


class BaseMlassifier:
    """
    Classe de base pour les mod√®les de classification multilabel.
    """
    
    def __init__(self, name: str = "BaseClassifier"):
        """
        Initialise le classificateur de base.
        
        Args:
            name (str): Nom du mod√®le
        """
        self.name = name
        self.mlb = None
        self.model = None
        self.is_fitted = False
    
    def fit(self, X, y):
        """
        Entra√Æne le mod√®le (√† impl√©menter dans les classes enfants).
        
        Args:
            X: Features d'entra√Ænement
            y: Labels d'entra√Ænement
        """
        raise NotImplementedError("M√©thode √† impl√©menter dans les classes enfants")
    
    def predict(self, X):
        """
        Pr√©dit les labels (√† impl√©menter dans les classes enfants).
        
        Args:
            X: Features pour la pr√©diction
            
        Returns:
            Pr√©dictions
        """
        raise NotImplementedError("M√©thode √† impl√©menter dans les classes enfants")
    
    def save(self, save_dir: str):
        """
        Sauvegarde le mod√®le (√† impl√©menter dans les classes enfants).
        
        Args:
            save_dir (str): R√©pertoire de sauvegarde
        """
        raise NotImplementedError("M√©thode √† impl√©menter dans les classes enfants")
    
    def load(self, save_dir: str):
        """
        Charge le mod√®le (√† impl√©menter dans les classes enfants).
        
        Args:
            save_dir (str): R√©pertoire contenant les mod√®les sauvegard√©s
        """
        raise NotImplementedError("M√©thode √† impl√©menter dans les classes enfants")


class TfidfLogRegClassifier(BaseMlassifier):
    """
    Classificateur multilabel utilisant TF-IDF + Logistic Regression.
    """
    
    def __init__(self, 
                 max_features: int = 30000,
                 ngram_range: Tuple[int, int] = (1, 3),
                 max_iter: int = 200,
                 min_df: int = 2,
                 max_df: float = 0.8):
        """
        Initialise le classificateur TF-IDF + LogReg.
        
        Args:
            max_features (int): Nombre maximum de features TF-IDF
            ngram_range (Tuple[int, int]): Range des n-grams (min, max)
            max_iter (int): Nombre maximum d'it√©rations pour LogReg
            min_df (int): Fr√©quence minimale de document
            max_df (float): Fr√©quence maximale de document (proportion)
        """
        super().__init__(name="TfidfLogReg")
        
        self.tfidf = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            sublinear_tf=True,
            min_df=min_df,
            max_df=max_df,
        )
        
        self.mlb = MultiLabelBinarizer()
        self.model = OneVsRestClassifier(LogisticRegression(max_iter=max_iter))
        self.max_iter = max_iter
    
    def fit(self, X_train, y_train):
        """
        Entra√Æne le mod√®le TF-IDF + LogReg.
        
        Args:
            X_train (pd.Series ou list): Textes d'entra√Ænement
            y_train (list of lists): Labels d'entra√Ænement (format multilabel)
            
        Returns:
            self: Objet entra√Æn√©
        """
        print(f"\nüîß Entra√Ænement du mod√®le {self.name}...")
        
        # Transformer les labels
        Y_train = self.mlb.fit_transform(y_train)
        print(f"üìä Classes d√©tect√©es: {list(self.mlb.classes_)}")
        
        # Vectorisation TF-IDF
        print("üìù Vectorisation TF-IDF...")
        X_train_tfidf = self.tfidf.fit_transform(X_train)
        print(f"‚úÖ Shape TF-IDF: {X_train_tfidf.shape}")
        
        # Entra√Ænement du mod√®le
        print("üéØ Entra√Ænement du classificateur...")
        self.model.fit(X_train_tfidf, Y_train)
        
        self.is_fitted = True
        print(f"‚úÖ Mod√®le {self.name} entra√Æn√© avec succ√®s")
        return self
    
    def predict(self, X_test):
        """
        Pr√©dit les labels pour de nouvelles donn√©es.
        
        Args:
            X_test (pd.Series ou list): Textes √† classifier
            
        Returns:
            np.ndarray: Pr√©dictions binaires (shape: n_samples x n_classes)
        """
        if not self.is_fitted:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant de faire des pr√©dictions")
        
        X_test_tfidf = self.tfidf.transform(X_test)
        Y_pred = self.model.predict(X_test_tfidf)
        return Y_pred
    
    def predict_proba(self, X_test):
        """
        Calcule les probabilit√©s de pr√©diction.
        
        Args:
            X_test (pd.Series ou list): Textes √† classifier
            
        Returns:
            np.ndarray: Probabilit√©s (shape: n_samples x n_classes)
        """
        if not self.is_fitted:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant de faire des pr√©dictions")
        
        X_test_tfidf = self.tfidf.transform(X_test)
        Y_proba = self.model.predict_proba(X_test_tfidf)
        return Y_proba
    
    def save(self, save_dir: str):
        """
        Sauvegarde le mod√®le et ses composants.
        
        Args:
            save_dir (str): R√©pertoire de sauvegarde
        """
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        with open(f"{save_dir}/tfidf_vectorizer.pkl", "wb") as f:
            pickle.dump(self.tfidf, f)
        
        with open(f"{save_dir}/logreg_model.pkl", "wb") as f:
            pickle.dump(self.model, f)
        
        with open(f"{save_dir}/label_binarizer.pkl", "wb") as f:
            pickle.dump(self.mlb, f)
        
        print(f"üíæ Mod√®le {self.name} sauvegard√© dans {save_dir}/")
    
    def load(self, save_dir: str):
        """
        Charge un mod√®le sauvegard√©.
        
        Args:
            save_dir (str): R√©pertoire contenant les mod√®les
        """
        with open(f"{save_dir}/tfidf_vectorizer.pkl", "rb") as f:
            self.tfidf = pickle.load(f)
        
        with open(f"{save_dir}/logreg_model.pkl", "rb") as f:
            self.model = pickle.load(f)
        
        with open(f"{save_dir}/label_binarizer.pkl", "rb") as f:
            self.mlb = pickle.load(f)
        
        self.is_fitted = True
        print(f"‚úÖ Mod√®le {self.name} charg√© depuis {save_dir}/")


class TfidfMiniLMClassifier(BaseMlassifier):
    """
    Classificateur multilabel utilisant TF-IDF + MiniLM embeddings + Logistic Regression.
    """
    
    def __init__(self, 
                 max_features: int = 30000,
                 ngram_range: Tuple[int, int] = (1, 3),
                 max_iter: int = 300,
                 min_df: int = 2,
                 max_df: float = 0.8,
                 embedding_model: str = 'all-MiniLM-L6-v2'):
        """
        Initialise le classificateur TF-IDF + MiniLM + LogReg.
        
        Args:
            max_features (int): Nombre maximum de features TF-IDF
            ngram_range (Tuple[int, int]): Range des n-grams (min, max)
            max_iter (int): Nombre maximum d'it√©rations pour LogReg
            min_df (int): Fr√©quence minimale de document
            max_df (float): Fr√©quence maximale de document
            embedding_model (str): Nom du mod√®le SentenceTransformer
        """
        super().__init__(name="TfidfMiniLM")
        
        self.tfidf = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            sublinear_tf=True,
            min_df=min_df,
            max_df=max_df,
        )
        
        self.embedding_model = SentenceTransformer(embedding_model)
        self.mlb = MultiLabelBinarizer()
        self.model = OneVsRestClassifier(LogisticRegression(max_iter=max_iter))
        self.embedding_model_name = embedding_model
    
    def fit(self, X_train, y_train):
        """
        Entra√Æne le mod√®le TF-IDF + MiniLM + LogReg.
        
        Args:
            X_train (pd.Series ou list): Textes d'entra√Ænement
            y_train (list of lists): Labels d'entra√Ænement
            
        Returns:
            self: Objet entra√Æn√©
        """
        print(f"\nüîß Entra√Ænement du mod√®le {self.name}...")
        
        # Transformer les labels
        Y_train = self.mlb.fit_transform(y_train)
        print(f"üìä Classes d√©tect√©es: {list(self.mlb.classes_)}")
        
        # Vectorisation TF-IDF
        print("üìù Vectorisation TF-IDF...")
        X_train_tfidf = self.tfidf.fit_transform(X_train)
        print(f"‚úÖ Shape TF-IDF: {X_train_tfidf.shape}")
        
        # G√©n√©ration des embeddings
        print("üß† G√©n√©ration des embeddings MiniLM...")
        X_train_list = X_train.tolist() if hasattr(X_train, 'tolist') else list(X_train)
        X_train_emb = self.embedding_model.encode(
            X_train_list, 
            show_progress_bar=True, 
            normalize_embeddings=True
        )
        X_train_emb = np.nan_to_num(X_train_emb)
        print(f"‚úÖ Shape embeddings: {X_train_emb.shape}")
        
        # Combinaison TF-IDF + embeddings
        print("üîó Combinaison des features...")
        X_train_combined = hstack([X_train_tfidf, X_train_emb])
        print(f"‚úÖ Shape combin√©e: {X_train_combined.shape}")
        
        # Entra√Ænement du mod√®le
        print("üéØ Entra√Ænement du classificateur...")
        self.model.fit(X_train_combined, Y_train)
        
        self.is_fitted = True
        print(f"‚úÖ Mod√®le {self.name} entra√Æn√© avec succ√®s")
        return self
    
    def predict(self, X_test):
        """
        Pr√©dit les labels pour de nouvelles donn√©es.
        
        Args:
            X_test (pd.Series ou list): Textes √† classifier
            
        Returns:
            np.ndarray: Pr√©dictions binaires
        """
        if not self.is_fitted:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant de faire des pr√©dictions")
        
        # TF-IDF
        X_test_tfidf = self.tfidf.transform(X_test)
        
        # Embeddings
        X_test_list = X_test.tolist() if hasattr(X_test, 'tolist') else list(X_test)
        X_test_emb = self.embedding_model.encode(
            X_test_list, 
            show_progress_bar=False, 
            normalize_embeddings=True
        )
        X_test_emb = np.nan_to_num(X_test_emb)
        
        # Combinaison
        X_test_combined = hstack([X_test_tfidf, X_test_emb])
        
        Y_pred = self.model.predict(X_test_combined)
        return Y_pred
    
    def predict_proba(self, X_test):
        """
        Calcule les probabilit√©s de pr√©diction.
        
        Args:
            X_test (pd.Series ou list): Textes √† classifier
            
        Returns:
            np.ndarray: Probabilit√©s
        """
        if not self.is_fitted:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant de faire des pr√©dictions")
        
        # TF-IDF
        X_test_tfidf = self.tfidf.transform(X_test)
        
        # Embeddings
        X_test_list = X_test.tolist() if hasattr(X_test, 'tolist') else list(X_test)
        X_test_emb = self.embedding_model.encode(
            X_test_list, 
            show_progress_bar=False, 
            normalize_embeddings=True
        )
        X_test_emb = np.nan_to_num(X_test_emb)
        
        # Combinaison
        X_test_combined = hstack([X_test_tfidf, X_test_emb])
        
        Y_proba = self.model.predict_proba(X_test_combined)
        return Y_proba
    
    def predict_with_threshold(self, X_test, thresholds: Dict[str, float]):
        """
        Pr√©dit avec des seuils personnalis√©s par classe.
        
        Args:
            X_test (pd.Series ou list): Textes √† classifier
            thresholds (Dict[str, float]): Dictionnaire {classe: seuil}
            
        Returns:
            np.ndarray: Pr√©dictions binaires avec seuils personnalis√©s
        """
        Y_proba = self.predict_proba(X_test)
        Y_pred_custom = np.zeros_like(Y_proba, dtype=int)
        
        for i, cls in enumerate(self.mlb.classes_):
            threshold = thresholds.get(cls, 0.5)
            Y_pred_custom[:, i] = (Y_proba[:, i] >= threshold).astype(int)
        
        return Y_pred_custom
    
    def save(self, save_dir: str):
        """
        Sauvegarde le mod√®le et ses composants.
        
        Args:
            save_dir (str): R√©pertoire de sauvegarde
        """
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        with open(f"{save_dir}/tfidf_vectorizer.pkl", "wb") as f:
            pickle.dump(self.tfidf, f)
        
        with open(f"{save_dir}/logreg_multilabel.pkl", "wb") as f:
            pickle.dump(self.model, f)
        
        with open(f"{save_dir}/label_binarizer.pkl", "wb") as f:
            pickle.dump(self.mlb, f)
        
        # Sauvegarder le nom du mod√®le d'embedding
        with open(f"{save_dir}/embedding_model_name.txt", "w") as f:
            f.write(self.embedding_model_name)
        
        print(f"üíæ Mod√®le {self.name} sauvegard√© dans {save_dir}/")
    
    def load(self, save_dir: str):
        """
        Charge un mod√®le sauvegard√©.
        
        Args:
            save_dir (str): R√©pertoire contenant les mod√®les
        """
        with open(f"{save_dir}/tfidf_vectorizer.pkl", "rb") as f:
            self.tfidf = pickle.load(f)
        
        with open(f"{save_dir}/logreg_multilabel.pkl", "rb") as f:
            self.model = pickle.load(f)
        
        with open(f"{save_dir}/label_binarizer.pkl", "rb") as f:
            self.mlb = pickle.load(f)
        
        # Charger le mod√®le d'embedding
        with open(f"{save_dir}/embedding_model_name.txt", "r") as f:
            embedding_model_name = f.read().strip()
        self.embedding_model = SentenceTransformer(embedding_model_name)
        
        self.is_fitted = True
        print(f"‚úÖ Mod√®le {self.name} charg√© depuis {save_dir}/")


if __name__ == "__main__":
    print("Module models.py - Classes de mod√®les disponibles:")
    print("- TfidfLogRegClassifier: TF-IDF + Logistic Regression")
    print("- TfidfMiniLMClassifier: TF-IDF + MiniLM + Logistic Regression")