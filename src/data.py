"""
Module de preprocessing des donnÃ©es pour la classification multilabel.
Contient les fonctions de chargement, nettoyage et transformation des donnÃ©es.
"""

import os
import json
import re
import pandas as pd
import nltk
from nltk.corpus import stopwords
from typing import List, Dict, Any


# Liste des tags cibles
TARGET_TAGS = [
    'math', 'graphs', 'strings', 
    'number theory', 'trees', 'geometry', 
    'games', 'probabilities'
]


def load_json_files(data_dir: str) -> pd.DataFrame:
    """
    Charge tous les fichiers JSON d'un dossier et les combine dans un DataFrame.
    
    Args:
        data_dir (str): Chemin du dossier contenant les fichiers JSON
        
    Returns:
        pd.DataFrame: DataFrame contenant tous les donnÃ©es chargÃ©es avec une colonne 'file_name'
        
    Raises:
        FileNotFoundError: Si le dossier n'existe pas
    """
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"Le dossier {data_dir} n'existe pas")
    
    rows = []
    
    for file_name in os.listdir(data_dir):
        if file_name.endswith(".json"):
            file_path = os.path.join(data_dir, file_name)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    first_line = f.readline().strip()
                    f.seek(0)
                    
                    if first_line.startswith("{"):
                        # Format JSON normal
                        data = json.load(f)
                        data["file_name"] = file_name
                        rows.append(data)
                    else:
                        # Format JSONL (plusieurs lignes)
                        for line in f:
                            if line.strip():
                                data = json.loads(line)
                                data["file_name"] = file_name
                                rows.append(data)
            except Exception as e:
                print(f"âš ï¸ Erreur avec {file_name}: {e}")
    
    df = pd.DataFrame(rows)
    print(f"âœ… {len(df)} exemples chargÃ©s depuis {len(rows)} fichiers")
    return df


def filter_tags(tag_list: List[str], target_tags: List[str] = TARGET_TAGS) -> List[str]:
    """
    Filtre une liste de tags pour ne garder que les tags cibles.
    
    Args:
        tag_list (List[str]): Liste de tags Ã  filtrer
        target_tags (List[str]): Liste des tags Ã  conserver (par dÃ©faut TARGET_TAGS)
        
    Returns:
        List[str]: Liste filtrÃ©e contenant uniquement les tags cibles
    """
    if not isinstance(tag_list, list):
        return []
    return [t for t in tag_list if t in target_tags]


def filter_dataset_by_tags(df: pd.DataFrame, tag_column: str = "tags") -> pd.DataFrame:
    """
    Filtre le dataset pour ne garder que les lignes contenant au moins un tag cible.
    
    Args:
        df (pd.DataFrame): DataFrame Ã  filtrer
        tag_column (str): Nom de la colonne contenant les tags (par dÃ©faut "tags")
        
    Returns:
        pd.DataFrame: DataFrame filtrÃ© avec une nouvelle colonne 'raw' contenant les tags filtrÃ©s
    """
    # CrÃ©er la colonne 'raw' avec les tags filtrÃ©s
    df["raw"] = df[tag_column].apply(filter_tags)
    
    # Garder uniquement les lignes avec au moins un tag cible
    df_filtered = df[~(df["raw"].isna() | df["raw"].apply(lambda x: len(x) == 0))].copy()
    
    print(f"âœ… {len(df_filtered)} exemples conservÃ©s aprÃ¨s filtrage des tags")
    return df_filtered


def download_stopwords():
    """
    TÃ©lÃ©charge les stopwords NLTK si nÃ©cessaire.
    
    Returns:
        None
    """
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        print("ğŸ“¥ TÃ©lÃ©chargement des stopwords...")
        nltk.download('stopwords', quiet=True)


def clean_text(text: str, stop_words: set = None) -> str:
    """
    Nettoie un texte : mise en minuscule, suppression de caractÃ¨res spÃ©ciaux et stopwords.
    
    Args:
        text (str): Texte Ã  nettoyer
        stop_words (set): Ensemble de stopwords Ã  supprimer (par dÃ©faut stopwords anglais)
        
    Returns:
        str: Texte nettoyÃ©
    """
    if stop_words is None:
        download_stopwords()
        stop_words = set(stopwords.words("english"))
    
    # Mise en minuscule
    text = text.lower()
    
    # Suppression des caractÃ¨res spÃ©ciaux (garder lettres, chiffres et espaces)
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    
    # Normalisation des espaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Suppression des stopwords
    text = " ".join([w for w in text.split() if w not in stop_words])
    
    return text


def combine_text_columns(df: pd.DataFrame, 
                         columns: List[str] = None) -> pd.DataFrame:
    """
    Combine plusieurs colonnes textuelles en une seule colonne.
    
    Args:
        df (pd.DataFrame): DataFrame contenant les colonnes Ã  combiner
        columns (List[str]): Liste des noms de colonnes Ã  combiner 
                            (par dÃ©faut: prob_desc_description, source_code, 
                             prob_desc_output_spec, prob_desc_input_spec)
        
    Returns:
        pd.DataFrame: DataFrame avec une nouvelle colonne 'combined_text'
    """
    if columns is None:
        columns = [
            'prob_desc_description',
            'source_code',
            'prob_desc_output_spec',
            'prob_desc_input_spec'
        ]
    
    # Combiner les colonnes en gÃ©rant les valeurs manquantes
    combined = df[columns[0]].fillna('')
    for col in columns[1:]:
        combined = combined + ' ' + df[col].fillna('')
    
    df['combined_text'] = combined
    print(f"âœ… Colonnes combinÃ©es: {', '.join(columns)}")
    return df


def preprocess_data(df: pd.DataFrame, 
                   text_columns: List[str] = None) -> pd.DataFrame:
    """
    Pipeline complet de preprocessing : filtrage des tags, combinaison et nettoyage du texte.
    
    Args:
        df (pd.DataFrame): DataFrame brut Ã  preprocesser
        text_columns (List[str]): Liste des colonnes Ã  combiner (optionnel)
        
    Returns:
        pd.DataFrame: DataFrame preprocessÃ© avec colonnes 'raw', 'combined_text' et 'clean_text'
    """
    print("\nğŸ”§ DÃ©but du preprocessing...")
    
    # Filtrer par tags
    df = filter_dataset_by_tags(df)
    
    # Combiner les colonnes textuelles
    df = combine_text_columns(df, text_columns)
    
    # TÃ©lÃ©charger les stopwords si nÃ©cessaire
    download_stopwords()
    stop_words = set(stopwords.words("english"))
    
    # Nettoyer le texte
    print("ğŸ§¹ Nettoyage du texte...")
    df["clean_text"] = df["combined_text"].fillna("").apply(
        lambda x: clean_text(x, stop_words)
    )
    
    print(f"âœ… Preprocessing terminÃ© - {len(df)} exemples prÃªts")
    return df


def augment_rare_classes(X_train: pd.Series, 
                         Y_train: pd.DataFrame, 
                         rare_tags: List[str],
                         multiplier: int = 2) -> tuple:
    """
    Augmente les donnÃ©es pour les classes rares par duplication.
    
    Args:
        X_train (pd.Series): Textes d'entraÃ®nement
        Y_train (pd.DataFrame): Labels d'entraÃ®nement (format one-hot)
        rare_tags (List[str]): Liste des tags rares Ã  augmenter
        multiplier (int): Facteur de multiplication (par dÃ©faut 2)
        
    Returns:
        tuple: (X_train_augmented, Y_train_augmented)
            - X_train_augmented (pd.Series): Textes augmentÃ©s
            - Y_train_augmented (np.ndarray): Labels augmentÃ©s
    """
    # CrÃ©er un DataFrame combinÃ©
    train_df = pd.DataFrame({"text": X_train}).reset_index(drop=True)
    Y_train_reset = Y_train.reset_index(drop=True)
    train_df = pd.concat([train_df, Y_train_reset], axis=1)
    
    # Augmenter chaque classe rare
    for tag in rare_tags:
        if tag in train_df.columns:
            rare_rows = train_df[train_df[tag] == 1]
            # Dupliquer les exemples rares
            train_df = pd.concat([train_df] + [rare_rows] * (multiplier - 1), 
                                ignore_index=True)
            print(f"ğŸ“ˆ Classe '{tag}' augmentÃ©e: {len(rare_rows)} â†’ {len(rare_rows) * multiplier} exemples")
    
    # SÃ©parer features et labels
    X_train_aug = train_df["text"]
    Y_train_aug = train_df[Y_train.columns].values
    
    print(f"âœ… Total aprÃ¨s augmentation: {len(X_train_aug)} exemples")
    return X_train_aug, Y_train_aug


if __name__ == "__main__":
    # Test du module
    print("Module data.py - Test des fonctions")
    
    # Test de nettoyage de texte
    sample_text = "This is a TEST with some SPECIAL characters!!! And stopwords."
    cleaned = clean_text(sample_text)
    print(f"\nTexte original: {sample_text}")
    print(f"Texte nettoyÃ©: {cleaned}")