"""
Module d'√©valuation des mod√®les de classification multilabel.
Contient les fonctions pour calculer les m√©triques et g√©n√©rer des visualisations.
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, 
    confusion_matrix,
    precision_recall_fscore_support,
    hamming_loss,
    accuracy_score
)
from fpdf import FPDF
from typing import Dict, List, Tuple, Optional


def compute_classification_report(Y_true: np.ndarray, 
                                  Y_pred: np.ndarray, 
                                  class_names: List[str],
                                  output_dict: bool = False) -> Dict:
    """
    Calcule le rapport de classification pour un probl√®me multilabel.
    
    Args:
        Y_true (np.ndarray): Labels r√©els (shape: n_samples x n_classes)
        Y_pred (np.ndarray): Labels pr√©dits (shape: n_samples x n_classes)
        class_names (List[str]): Noms des classes
        output_dict (bool): Si True, retourne un dictionnaire au lieu d'une string
        
    Returns:
        Dict ou str: Rapport de classification
    """
    report = classification_report(
        Y_true, 
        Y_pred, 
        target_names=class_names,
        output_dict=output_dict,
        zero_division=0
    )
    return report


def compute_additional_metrics(Y_true: np.ndarray, Y_pred: np.ndarray) -> Dict[str, float]:
    """
    Calcule des m√©triques additionnelles pour la classification multilabel.
    
    Args:
        Y_true (np.ndarray): Labels r√©els
        Y_pred (np.ndarray): Labels pr√©dits
        
    Returns:
        Dict[str, float]: Dictionnaire contenant:
            - hamming_loss: Proportion d'erreurs sur l'ensemble des labels
            - exact_match_ratio: Proportion de pr√©dictions parfaitement correctes
            - accuracy_score: Score d'exactitude subset
    """
    metrics = {
        'hamming_loss': hamming_loss(Y_true, Y_pred),
        'exact_match_ratio': accuracy_score(Y_true, Y_pred),
    }
    
    return metrics


def compute_per_class_metrics(Y_true: np.ndarray, 
                              Y_pred: np.ndarray,
                              class_names: List[str]) -> pd.DataFrame:
    """
    Calcule les m√©triques par classe (precision, recall, f1-score, support).
    
    Args:
        Y_true (np.ndarray): Labels r√©els
        Y_pred (np.ndarray): Labels pr√©dits
        class_names (List[str]): Noms des classes
        
    Returns:
        pd.DataFrame: DataFrame avec les m√©triques par classe
    """
    precision, recall, f1, support = precision_recall_fscore_support(
        Y_true, Y_pred, average=None, zero_division=0
    )
    
    df_metrics = pd.DataFrame({
        'Class': class_names,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'Support': support
    })
    
    return df_metrics


def generate_confusion_matrices(Y_true: np.ndarray, 
                                Y_pred: np.ndarray,
                                class_names: List[str],
                                output_dir: str = "confusion_matrices") -> List[str]:
    """
    G√©n√®re et sauvegarde les matrices de confusion pour chaque classe.
    
    Args:
        Y_true (np.ndarray): Labels r√©els
        Y_pred (np.ndarray): Labels pr√©dits
        class_names (List[str]): Noms des classes
        output_dir (str): R√©pertoire de sauvegarde des images
        
    Returns:
        List[str]: Liste des chemins des images g√©n√©r√©es
    """
    os.makedirs(output_dir, exist_ok=True)
    image_paths = []
    
    for idx, label in enumerate(class_names):
        cm = confusion_matrix(Y_true[:, idx], Y_pred[:, idx])
        
        plt.figure(figsize=(4, 3))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                   xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
        plt.title(f"Confusion Matrix: {label}")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.tight_layout()
        
        img_path = os.path.join(output_dir, f"cm_{label}.png")
        plt.savefig(img_path, dpi=100, bbox_inches='tight')
        plt.close()
        
        image_paths.append(img_path)
    
    print(f"‚úÖ {len(image_paths)} matrices de confusion g√©n√©r√©es dans {output_dir}/")
    return image_paths


def generate_metrics_summary_plot(metrics_df: pd.DataFrame, 
                                  output_path: str = "metrics_summary.png"):
    """
    G√©n√®re un graphique r√©sumant les m√©triques par classe.
    
    Args:
        metrics_df (pd.DataFrame): DataFrame contenant les m√©triques par classe
        output_path (str): Chemin de sauvegarde du graphique
        
    Returns:
        str: Chemin du fichier g√©n√©r√©
    """
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(metrics_df))
    width = 0.25
    
    ax.bar(x - width, metrics_df['Precision'], width, label='Precision', alpha=0.8)
    ax.bar(x, metrics_df['Recall'], width, label='Recall', alpha=0.8)
    ax.bar(x + width, metrics_df['F1-Score'], width, label='F1-Score', alpha=0.8)
    
    ax.set_xlabel('Classes')
    ax.set_ylabel('Scores')
    ax.set_title('M√©triques par classe')
    ax.set_xticks(x)
    ax.set_xticklabels(metrics_df['Class'], rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"‚úÖ Graphique de synth√®se sauvegard√©: {output_path}")
    return output_path


def generate_pdf_report(report_text: str,
                       class_names: List[str],
                       additional_metrics: Dict[str, float],
                       confusion_matrices_dir: str,
                       output_path: str = "classification_report.pdf",
                       model_name: str = "Model"):
    """
    G√©n√®re un rapport PDF complet avec m√©triques et matrices de confusion.
    
    Args:
        report_text (str): Texte du rapport de classification
        class_names (List[str]): Noms des classes
        additional_metrics (Dict[str, float]): M√©triques additionnelles
        confusion_matrices_dir (str): R√©pertoire contenant les matrices de confusion
        output_path (str): Chemin du fichier PDF √† g√©n√©rer
        model_name (str): Nom du mod√®le pour le titre
        
    Returns:
        str: Chemin du fichier PDF g√©n√©r√©
    """
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    
    # Page de titre
    pdf.add_page()
    pdf.set_font("Arial", "B", 18)
    pdf.cell(0, 10, f"Rapport de Classification - {model_name}", ln=True, align="C")
    pdf.ln(10)
    
    # M√©triques globales
    pdf.set_font("Arial", "B", 14)
    pdf.cell(0, 10, "M√©triques Globales", ln=True)
    pdf.set_font("Arial", "", 11)
    
    for metric_name, metric_value in additional_metrics.items():
        metric_display = metric_name.replace('_', ' ').title()
        pdf.cell(0, 8, f"{metric_display}: {metric_value:.4f}", ln=True)
    
    pdf.ln(5)
    
    # Rapport de classification d√©taill√©
    pdf.set_font("Arial", "B", 14)
    pdf.cell(0, 10, "Rapport de Classification D√©taill√©", ln=True)
    pdf.set_font("Courier", "", 9)
    
    # Diviser le rapport en lignes pour √©viter les d√©bordements
    for line in report_text.split('\n'):
        # Tronquer les lignes trop longues
        if len(line) > 80:
            line = line[:77] + "..."
        pdf.cell(0, 5, line, ln=True)
    
    # Matrices de confusion
    pdf.add_page()
    pdf.set_font("Arial", "B", 14)
    pdf.cell(0, 10, "Matrices de Confusion", ln=True, align="C")
    pdf.ln(5)
    
    for label in class_names:
        img_path = os.path.join(confusion_matrices_dir, f"cm_{label}.png")
        if os.path.exists(img_path):
            pdf.set_font("Arial", "B", 12)
            pdf.cell(0, 8, f"Classe: {label}", ln=True)
            
            # Ajouter l'image avec une taille appropri√©e
            try:
                pdf.image(img_path, w=100)
                pdf.ln(5)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de l'ajout de l'image {img_path}: {e}")
    
    # Sauvegarder le PDF
    pdf.output(output_path)
    print(f"üìÑ Rapport PDF g√©n√©r√©: {output_path}")
    return output_path


def evaluate_model(model, 
                  X_test, 
                  Y_test,
                  class_names: List[str],
                  output_dir: str = "reports",
                  model_name: str = "Model") -> Dict:
    """
    Pipeline complet d'√©valuation d'un mod√®le.
    
    Args:
        model: Mod√®le entra√Æn√© avec m√©thode predict()
        X_test: Features de test
        Y_test (np.ndarray): Labels de test
        class_names (List[str]): Noms des classes
        output_dir (str): R√©pertoire de sortie pour les rapports
        model_name (str): Nom du mod√®le
        
    Returns:
        Dict: Dictionnaire contenant toutes les m√©triques et chemins des fichiers g√©n√©r√©s
    """
    print(f"\nüìä √âvaluation du mod√®le {model_name}...")
    
    # Cr√©er le r√©pertoire de sortie
    os.makedirs(output_dir, exist_ok=True)
    
    # Pr√©dictions
    print("üîÆ G√©n√©ration des pr√©dictions...")
    Y_pred = model.predict(X_test)
    
    # Rapport de classification
    print("üìù Calcul des m√©triques...")
    report_dict = compute_classification_report(Y_test, Y_pred, class_names, output_dict=True)
    report_text = compute_classification_report(Y_test, Y_pred, class_names, output_dict=False)
    
    # M√©triques additionnelles
    additional_metrics = compute_additional_metrics(Y_test, Y_pred)
    
    # M√©triques par classe
    metrics_df = compute_per_class_metrics(Y_test, Y_pred, class_names)
    metrics_csv_path = os.path.join(output_dir, "metrics_per_class.csv")
    metrics_df.to_csv(metrics_csv_path, index=False)
    
    # G√©n√©rer le graphique de synth√®se
    summary_plot_path = os.path.join(output_dir, "metrics_summary.png")
    generate_metrics_summary_plot(metrics_df, summary_plot_path)
    
    # G√©n√©rer les matrices de confusion
    cm_dir = os.path.join(output_dir, "confusion_matrices")
    cm_paths = generate_confusion_matrices(Y_test, Y_pred, class_names, cm_dir)
    
    # G√©n√©rer le rapport PDF
    pdf_path = os.path.join(output_dir, f"report_{model_name}.pdf")
    generate_pdf_report(
        report_text, 
        class_names, 
        additional_metrics, 
        cm_dir, 
        pdf_path, 
        model_name
    )
    
    # Afficher les r√©sultats
    print("\n" + "="*60)
    print("R√âSULTATS DE L'√âVALUATION")
    print("="*60)
    print(report_text)
    print("\nM√©triques additionnelles:")
    for metric_name, metric_value in additional_metrics.items():
        print(f"  {metric_name}: {metric_value:.4f}")
    print("="*60 + "\n")
    
    # Retourner toutes les informations
    results = {
        'report_dict': report_dict,
        'report_text': report_text,
        'additional_metrics': additional_metrics,
        'metrics_df': metrics_df,
        'metrics_csv_path': metrics_csv_path,
        'summary_plot_path': summary_plot_path,
        'confusion_matrices_dir': cm_dir,
        'pdf_path': pdf_path,
        'Y_pred': Y_pred
    }
    
    return results


if __name__ == "__main__":
    print("Module evaluation.py - Fonctions d'√©valuation disponibles")
    print("- compute_classification_report()")
    print("- compute_additional_metrics()")
    print("- compute_per_class_metrics()")
    print("- generate_confusion_matrices()")
    print("- generate_pdf_report()")
    print("- evaluate_model() [Pipeline complet]")