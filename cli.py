#!/usr/bin/env python3
"""
Interface en ligne de commande (CLI) pour l'entra√Ænement et l'√©valuation
des mod√®les de classification multilabel.
"""

import argparse
import sys
import os

# Ajouter le dossier src au path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from train_model import train_tfidf_logreg, train_tfidf_minilm, compare_models


def main():
    """
    Point d'entr√©e principal de la CLI.
    """
    parser = argparse.ArgumentParser(
        description="CLI pour l'entra√Ænement de mod√®les de classification multilabel",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  # Entra√Æner le mod√®le TF-IDF + LogReg
  python cli.py train --model tfidf-logreg --data-dir ./data
  
  # Entra√Æner le mod√®le TF-IDF + MiniLM avec augmentation
  python cli.py train --model tfidf-minilm --data-dir ./data --augment
  
  # Comparer les deux mod√®les
  python cli.py compare --data-dir ./data
  
  # Entra√Æner avec param√®tres personnalis√©s
  python cli.py train --model tfidf-logreg --data-dir ./data --test-size 0.3 --max-iter 300
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Commandes disponibles')
    
    # ============================================================
    # Commande: train
    # ============================================================
    train_parser = subparsers.add_parser(
        'train',
        help='Entra√Æner un mod√®le de classification'
    )
    
    train_parser.add_argument(
        '--model',
        type=str,
        required=True,
        choices=['tfidf-logreg', 'tfidf-minilm'],
        help='Type de mod√®le √† entra√Æner'
    )
    
    train_parser.add_argument(
        '--data-dir',
        type=str,
        required=True,
        help='R√©pertoire contenant les fichiers JSON de donn√©es'
    )
    
    train_parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='Proportion du jeu de test (par d√©faut: 0.2)'
    )
    
    train_parser.add_argument(
        '--random-state',
        type=int,
        default=42,
        help='Seed pour la reproductibilit√© (par d√©faut: 42)'
    )
    
    train_parser.add_argument(
        '--save-dir',
        type=str,
        default=None,
        help='R√©pertoire de sauvegarde du mod√®le (par d√©faut: saved_models/<model_name>)'
    )
    
    train_parser.add_argument(
        '--report-dir',
        type=str,
        default=None,
        help='R√©pertoire pour les rapports (par d√©faut: reports/<model_name>)'
    )
    
    # Param√®tres TF-IDF
    train_parser.add_argument(
        '--max-features',
        type=int,
        default=30000,
        help='Nombre max de features TF-IDF (par d√©faut: 30000)'
    )
    
    train_parser.add_argument(
        '--max-iter',
        type=int,
        default=None,
        help='Nombre max d\'it√©rations LogReg (par d√©faut: 200 pour tfidf-logreg, 300 pour tfidf-minilm)'
    )
    
    # Param√®tres pour TF-IDF + MiniLM
    train_parser.add_argument(
        '--augment',
        action='store_true',
        help='Activer l\'augmentation des classes rares (uniquement pour tfidf-minilm)'
    )
    
    train_parser.add_argument(
        '--rare-tags',
        type=str,
        nargs='+',
        default=None,
        help='Liste des tags rares √† augmenter (par d√©faut: probabilities games geometry)'
    )
    
    train_parser.add_argument(
        '--augment-multiplier',
        type=int,
        default=2,
        help='Facteur de multiplication pour l\'augmentation (par d√©faut: 2)'
    )
    
    train_parser.add_argument(
        '--custom-thresholds',
        action='store_true',
        help='Utiliser des seuils personnalis√©s pour les classes rares (uniquement pour tfidf-minilm)'
    )
    
    # ============================================================
    # Commande: compare
    # ============================================================
    compare_parser = subparsers.add_parser(
        'compare',
        help='Comparer les performances de plusieurs mod√®les'
    )
    
    compare_parser.add_argument(
        '--data-dir',
        type=str,
        required=True,
        help='R√©pertoire contenant les fichiers JSON de donn√©es'
    )
    
    compare_parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='Proportion du jeu de test (par d√©faut: 0.2)'
    )
    
    compare_parser.add_argument(
        '--random-state',
        type=int,
        default=42,
        help='Seed pour la reproductibilit√© (par d√©faut: 42)'
    )
    
    # ============================================================
    # Parsing des arguments
    # ============================================================
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        sys.exit(1)
    
    # ============================================================
    # Ex√©cution des commandes
    # ============================================================
    if args.command == 'train':
        execute_train(args)
    elif args.command == 'compare':
        execute_compare(args)


def execute_train(args):
    """
    Ex√©cute la commande d'entra√Ænement.
    
    Args:
        args: Arguments pars√©s depuis la CLI
    """
    print("\n" + "="*70)
    print(f"LANCEMENT DE L'ENTRA√éNEMENT: {args.model.upper()}")
    print("="*70)
    print(f"üìÇ R√©pertoire de donn√©es: {args.data_dir}")
    print(f"üîÄ Test size: {args.test_size}")
    print(f"üé≤ Random state: {args.random_state}")
    
    # V√©rifier que le r√©pertoire de donn√©es existe
    if not os.path.exists(args.data_dir):
        print(f"\n‚ùå Erreur: Le r√©pertoire {args.data_dir} n'existe pas")
        sys.exit(1)
    
    # Pr√©parer les param√®tres du mod√®le
    model_params = {
        'max_features': args.max_features,
    }
    
    if args.max_iter is not None:
        model_params['max_iter'] = args.max_iter
    
    try:
        if args.model == 'tfidf-logreg':
            # D√©finir les r√©pertoires par d√©faut
            save_dir = args.save_dir or "saved_models/tfidf_logreg"
            report_dir = args.report_dir or "reports/tfidf_logreg"
            
            if args.max_iter is None:
                model_params['max_iter'] = 200
            
            results = train_tfidf_logreg(
                data_dir=args.data_dir,
                test_size=args.test_size,
                random_state=args.random_state,
                save_dir=save_dir,
                report_dir=report_dir,
                **model_params
            )
            
        elif args.model == 'tfidf-minilm':
            # D√©finir les r√©pertoires par d√©faut
            save_dir = args.save_dir or "saved_models/tfidf_minilm"
            report_dir = args.report_dir or "reports/tfidf_minilm"
            
            if args.max_iter is None:
                model_params['max_iter'] = 300
            
            rare_tags = args.rare_tags or ["probabilities", "games", "geometry"]
            
            results = train_tfidf_minilm(
                data_dir=args.data_dir,
                test_size=args.test_size,
                random_state=args.random_state,
                augment_rare=args.augment,
                rare_tags=rare_tags,
                augment_multiplier=args.augment_multiplier,
                use_custom_thresholds=args.custom_thresholds,
                save_dir=save_dir,
                report_dir=report_dir,
                **model_params
            )
        
        print("\n" + "="*70)
        print("‚úÖ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!")
        print("="*70)
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'entra√Ænement: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def execute_compare(args):
    """
    Ex√©cute la commande de comparaison.
    
    Args:
        args: Arguments pars√©s depuis la CLI
    """
    print("\n" + "="*70)
    print("LANCEMENT DE LA COMPARAISON DES MOD√àLES")
    print("="*70)
    print(f"üìÇ R√©pertoire de donn√©es: {args.data_dir}")
    
    # V√©rifier que le r√©pertoire de donn√©es existe
    if not os.path.exists(args.data_dir):
        print(f"\n‚ùå Erreur: Le r√©pertoire {args.data_dir} n'existe pas")
        sys.exit(1)
    
    try:
        comparison_df = compare_models(
            data_dir=args.data_dir,
            test_size=args.test_size,
            random_state=args.random_state
        )
        
        print("\n" + "="*70)
        print("‚úÖ COMPARAISON TERMIN√âE AVEC SUCC√àS!")
        print("="*70)
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de la comparaison: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()