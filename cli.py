#!/usr/bin/env python3
"""
Interface en ligne de commande (CLI) pour l'entra√Ænement et l'√©valuation
des mod√®les de classification multilabel.
"""

import argparse
import sys
import os
import json

# Ajouter le dossier src au path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from train_model import train_tfidf_logreg, train_tfidf_minilm, compare_models, predict_questions


def main():
    """
    Point d'entr√©e principal de la CLI.
    """
    parser = argparse.ArgumentParser(
        description="CLI pour l'entra√Ænement de mod√®les de classification multilabel",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  # Entra√Æner le mod√®le TF-IDF + LogReg
  python cli.py train --model tfidf-logreg --data-dir ./data
  
  # Entra√Æner le mod√®le TF-IDF + MiniLM avec augmentation
  python cli.py train --model tfidf-minilm --data-dir ./data --augment
  
  # Comparer les deux mod√®les
  python cli.py compare --data-dir ./data
  
  # Pr√©dire sur de nouvelles questions
  python cli.py predict --model-dir ./saved_models/tfidf_logreg --questions "What is the derivative of x^2?" "How to solve linear equations?"
  
  # Pr√©dire depuis un fichier JSON
  python cli.py predict --model-dir ./saved_models/tfidf_minilm --input-file questions.json --output-file predictions.json
  
  # Entra√Æner avec param√®tres personnalis√©s
  python cli.py train --model tfidf-logreg --data-dir ./data --test-size 0.3 --max-iter 300
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Commandes disponibles')
    
    # ============================================================
    # Commande: train
    # ============================================================
    train_parser = subparsers.add_parser(
        'train',
        help='Entra√Æner un mod√®le de classification'
    )
    
    train_parser.add_argument(
        '--model',
        type=str,
        required=True,
        choices=['tfidf-logreg', 'tfidf-minilm'],
        help='Type de mod√®le √† entra√Æner'
    )
    
    train_parser.add_argument(
        '--data-dir',
        type=str,
        required=True,
        help='R√©pertoire contenant les fichiers JSON de donn√©es'
    )
    
    train_parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='Proportion du jeu de test (par d√©faut: 0.2)'
    )
    
    train_parser.add_argument(
        '--random-state',
        type=int,
        default=42,
        help='Seed pour la reproductibilit√© (par d√©faut: 42)'
    )
    
    train_parser.add_argument(
        '--save-dir',
        type=str,
        default=None,
        help='R√©pertoire de sauvegarde du mod√®le (par d√©faut: saved_models/<model_name>)'
    )
    
    train_parser.add_argument(
        '--report-dir',
        type=str,
        default=None,
        help='R√©pertoire pour les rapports (par d√©faut: reports/<model_name>)'
    )
    
    # Param√®tres TF-IDF
    train_parser.add_argument(
        '--max-features',
        type=int,
        default=30000,
        help='Nombre max de features TF-IDF (par d√©faut: 30000)'
    )
    
    train_parser.add_argument(
        '--max-iter',
        type=int,
        default=None,
        help='Nombre max d\'it√©rations LogReg (par d√©faut: 200 pour tfidf-logreg, 300 pour tfidf-minilm)'
    )
    
    # Param√®tres pour TF-IDF + MiniLM
    train_parser.add_argument(
        '--augment',
        action='store_true',
        help='Activer l\'augmentation des classes rares (uniquement pour tfidf-minilm)'
    )
    
    train_parser.add_argument(
        '--rare-tags',
        type=str,
        nargs='+',
        default=None,
        help='Liste des tags rares √† augmenter (par d√©faut: probabilities games geometry)'
    )
    
    train_parser.add_argument(
        '--augment-multiplier',
        type=int,
        default=2,
        help='Facteur de multiplication pour l\'augmentation (par d√©faut: 2)'
    )
    
    train_parser.add_argument(
        '--custom-thresholds',
        action='store_true',
        help='Utiliser des seuils personnalis√©s pour les classes rares (uniquement pour tfidf-minilm)'
    )
    
    # ============================================================
    # Commande: compare
    # ============================================================
    compare_parser = subparsers.add_parser(
        'compare',
        help='Comparer les performances de plusieurs mod√®les'
    )
    
    compare_parser.add_argument(
        '--data-dir',
        type=str,
        required=True,
        help='R√©pertoire contenant les fichiers JSON de donn√©es'
    )
    
    compare_parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='Proportion du jeu de test (par d√©faut: 0.2)'
    )
    
    compare_parser.add_argument(
        '--random-state',
        type=int,
        default=42,
        help='Seed pour la reproductibilit√© (par d√©faut: 42)'
    )
    
    # ============================================================
    # Commande: predict
    # ============================================================
    predict_parser = subparsers.add_parser(
        'predict',
        help='Pr√©dire les tags pour de nouvelles questions'
    )
    
    predict_parser.add_argument(
        '--model-dir',
        type=str,
        required=True,
        help='R√©pertoire contenant le mod√®le sauvegard√©'
    )
    
    # Groupe mutuellement exclusif: questions directes OU fichier d'entr√©e
    input_group = predict_parser.add_mutually_exclusive_group(required=True)
    
    input_group.add_argument(
        '--questions',
        type=str,
        nargs='+',
        help='Une ou plusieurs questions √† pr√©dire (s√©par√©es par des espaces)'
    )
    
    input_group.add_argument(
        '--input-file',
        type=str,
        help='Fichier JSON contenant les questions (format: liste de strings ou liste de dicts avec cl√© "question")'
    )
    
    predict_parser.add_argument(
        '--output-file',
        type=str,
        default=None,
        help='Fichier JSON de sortie pour sauvegarder les pr√©dictions (optionnel)'
    )
    
    predict_parser.add_argument(
        '--threshold',
        type=float,
        default=None,
        help='Seuil de probabilit√© pour les pr√©dictions (par d√©faut: 0.5 ou seuils personnalis√©s du mod√®le)'
    )
    
    predict_parser.add_argument(
        '--top-k',
        type=int,
        default=None,
        help='Nombre maximum de tags √† retourner par question (par d√©faut: tous les tags au-dessus du seuil)'
    )
    
    predict_parser.add_argument(
        '--verbose',
        action='store_true',
        help='Afficher les probabilit√©s pour chaque pr√©diction'
    )
    
    # ============================================================
    # Parsing des arguments
    # ============================================================
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        sys.exit(1)
    
    # ============================================================
    # Ex√©cution des commandes
    # ============================================================
    if args.command == 'train':
        execute_train(args)
    elif args.command == 'compare':
        execute_compare(args)
    elif args.command == 'predict':
        execute_predict(args)


def execute_train(args):
    """
    Ex√©cute la commande d'entra√Ænement.
    
    Args:
        args: Arguments pars√©s depuis la CLI
    """
    print("\n" + "="*70)
    print(f"LANCEMENT DE L'ENTRA√éNEMENT: {args.model.upper()}")
    print("="*70)
    print(f"üìÇ R√©pertoire de donn√©es: {args.data_dir}")
    print(f"üîÄ Test size: {args.test_size}")
    print(f"üé≤ Random state: {args.random_state}")
    
    # V√©rifier que le r√©pertoire de donn√©es existe
    if not os.path.exists(args.data_dir):
        print(f"\n‚ùå Erreur: Le r√©pertoire {args.data_dir} n'existe pas")
        sys.exit(1)
    
    # Pr√©parer les param√®tres du mod√®le
    model_params = {
        'max_features': args.max_features,
    }
    
    if args.max_iter is not None:
        model_params['max_iter'] = args.max_iter
    
    try:
        if args.model == 'tfidf-logreg':
            # D√©finir les r√©pertoires par d√©faut
            save_dir = args.save_dir or "saved_models/tfidf_logreg"
            report_dir = args.report_dir or "reports/tfidf_logreg"
            
            if args.max_iter is None:
                model_params['max_iter'] = 200
            
            results = train_tfidf_logreg(
                data_dir=args.data_dir,
                test_size=args.test_size,
                random_state=args.random_state,
                save_dir=save_dir,
                report_dir=report_dir,
                **model_params
            )
            
        elif args.model == 'tfidf-minilm':
            # D√©finir les r√©pertoires par d√©faut
            save_dir = args.save_dir or "saved_models/tfidf_minilm"
            report_dir = args.report_dir or "reports/tfidf_minilm"
            
            if args.max_iter is None:
                model_params['max_iter'] = 300
            
            rare_tags = args.rare_tags or ["probabilities", "games", "geometry"]
            
            results = train_tfidf_minilm(
                data_dir=args.data_dir,
                test_size=args.test_size,
                random_state=args.random_state,
                augment_rare=args.augment,
                rare_tags=rare_tags,
                augment_multiplier=args.augment_multiplier,
                use_custom_thresholds=args.custom_thresholds,
                save_dir=save_dir,
                report_dir=report_dir,
                **model_params
            )
        
        print("\n" + "="*70)
        print("‚úÖ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!")
        print("="*70)
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'entra√Ænement: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def execute_compare(args):
    """
    Ex√©cute la commande de comparaison.
    
    Args:
        args: Arguments pars√©s depuis la CLI
    """
    print("\n" + "="*70)
    print("LANCEMENT DE LA COMPARAISON DES MOD√àLES")
    print("="*70)
    print(f"üìÇ R√©pertoire de donn√©es: {args.data_dir}")
    
    # V√©rifier que le r√©pertoire de donn√©es existe
    if not os.path.exists(args.data_dir):
        print(f"\n‚ùå Erreur: Le r√©pertoire {args.data_dir} n'existe pas")
        sys.exit(1)
    
    try:
        comparison_df = compare_models(
            data_dir=args.data_dir,
            test_size=args.test_size,
            random_state=args.random_state
        )
        
        print("\n" + "="*70)
        print("‚úÖ COMPARAISON TERMIN√âE AVEC SUCC√àS!")
        print("="*70)
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de la comparaison: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def execute_predict(args):
    """
    Ex√©cute la commande de pr√©diction.
    
    Args:
        args: Arguments pars√©s depuis la CLI
    """
    print("\n" + "="*70)
    print("LANCEMENT DE LA PR√âDICTION")
    print("="*70)
    print(f"üìÇ R√©pertoire du mod√®le: {args.model_dir}")
    
    # V√©rifier que le r√©pertoire du mod√®le existe
    if not os.path.exists(args.model_dir):
        print(f"\n‚ùå Erreur: Le r√©pertoire {args.model_dir} n'existe pas")
        sys.exit(1)
    
    try:
        # Charger les questions
        if args.questions:
            questions = args.questions
            print(f"üìù Nombre de questions: {len(questions)}")
        else:
            # Charger depuis un fichier
            print(f"üìÑ Chargement depuis: {args.input_file}")
            with open(args.input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # G√©rer diff√©rents formats
            if isinstance(data, list):
                if len(data) > 0 and isinstance(data[0], dict):
                    # Format: [{"question": "...", ...}, ...]
                    questions = [item.get('question', item.get('text', '')) for item in data]
                else:
                    # Format: ["question1", "question2", ...]
                    questions = data
            else:
                print(f"\n‚ùå Erreur: Format de fichier non reconnu. Attendu: liste de strings ou liste de dicts")
                sys.exit(1)
            
            print(f"üìù Nombre de questions charg√©es: {len(questions)}")
        
        # Effectuer les pr√©dictions
        print("\nüîÆ Pr√©diction en cours...")
        predictions = predict_questions(
            questions=questions,
            model_dir=args.model_dir,
            threshold=args.threshold,
            top_k=args.top_k
        )
        
        # Afficher les r√©sultats
        print("\n" + "="*70)
        print("R√âSULTATS DES PR√âDICTIONS")
        print("="*70)
        
        for i, (question, pred) in enumerate(zip(questions, predictions), 1):
            print(f"\n{'‚îÄ'*70}")
            print(f"Question {i}: {question[:100]}{'...' if len(question) > 100 else ''}")
            print(f"{'‚îÄ'*70}")
            
            tags = pred['tags']
            if tags:
                print(f"üè∑Ô∏è  Tags pr√©dits: {', '.join(tags)}")
                
                if args.verbose and 'probabilities' in pred:
                    print("\nüìä Probabilit√©s:")
                    probs = pred['probabilities']
                    for tag in tags:
                        if tag in probs:
                            print(f"   ‚Ä¢ {tag}: {probs[tag]:.4f}")
            else:
                print("üè∑Ô∏è  Aucun tag pr√©dit")
        
        # Sauvegarder les r√©sultats si demand√©
        if args.output_file:
            output_data = []
            for question, pred in zip(questions, predictions):
                output_item = {
                    'question': question,
                    'predicted_tags': pred['tags']
                }
                if 'probabilities' in pred:
                    output_item['probabilities'] = pred['probabilities']
                output_data.append(output_item)
            
            with open(args.output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nüíæ Pr√©dictions sauvegard√©es dans: {args.output_file}")
        
        print("\n" + "="*70)
        print("‚úÖ PR√âDICTION TERMIN√âE AVEC SUCC√àS!")
        print("="*70)
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de la pr√©diction: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()